{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Part_A_Q3c_Adam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9-I2_2G-wHC",
        "outputId": "30f6a7c8-eb2c-4b6b-971e-79d44fe8cd62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "# import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97sjozIa_DFZ",
        "outputId": "efb15674-349a-47eb-cbc9-05e15e41b718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# RTX On\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_sa0uV5_FYM"
      },
      "source": [
        "def load_data(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        try:\n",
        "            samples = pickle.load(fo)\n",
        "        except UnicodeDecodeError:  # python 3.x\n",
        "            fo.seek(0)\n",
        "            samples = pickle.load(fo, encoding='latin1')\n",
        "\n",
        "    data, labels = samples['data'], samples['labels']\n",
        "\n",
        "    data = np.array(data, dtype=np.float32) / 255\n",
        "    labels = np.array(labels, dtype=np.int32)\n",
        "    return data, labels"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCoRVIvO_7_q"
      },
      "source": [
        "def make_model(num_ch_c1, num_ch_c2, use_dropout):\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    if (use_dropout == False):\n",
        "        # Input Layer\n",
        "        model.add(layers.Input(shape=(3072,)))\n",
        "        model.add(layers.Reshape(target_shape=(32, 32, 3), input_shape=(3072,)))\n",
        "        # C₁ Convolutional Layer\n",
        "        model.add(layers.Conv2D(num_ch_c1, 9, padding='valid', activation='relu', input_shape=(None, None, 3)))\n",
        "        # S₁ Max Pooling Layer\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2), padding='valid'))\n",
        "        # C₂ Convolutional Layer\n",
        "        model.add(layers.Conv2D(num_ch_c2, 5, padding='valid', activation='relu', input_shape=(None, None, 3)))\n",
        "        # S₂ Max Pooling Layer\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2), padding='valid'))\n",
        "        # Flatten\n",
        "        model.add(layers.Flatten())\n",
        "        # F₃ Fully-Connected Layer\n",
        "        model.add(layers.Dense(300, use_bias=True))  # Here no softmax because we have combined it with the loss   \n",
        "        # F₄ Fully-Connected Layer \n",
        "        model.add(layers.Dense(10, use_bias=True, input_shape=(10,)))  # Here no softmax because we have combined it with the loss\n",
        "    else:\n",
        "        # Input Layer\n",
        "        model.add(layers.Input(shape=(3072,)))\n",
        "        model.add(layers.Reshape(target_shape=(32, 32, 3), input_shape=(3072,)))\n",
        "        # C₁ Convolutional Layer\n",
        "        model.add(layers.Conv2D(num_ch_c1, 9, padding='valid', activation='relu', input_shape=(None, None, 3)))\n",
        "        # S₁ Max Pooling Layer\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2), padding='valid'))\n",
        "        # C₂ Convolutional Layer\n",
        "        model.add(layers.Conv2D(num_ch_c2, 5, padding='valid', activation='relu', input_shape=(None, None, 3)))\n",
        "        # S₂ Max Pooling Layer\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2), padding='valid'))\n",
        "        # Flatten\n",
        "        model.add(layers.Flatten())\n",
        "        # F₃ Fully-Connected Layer\n",
        "        model.add(layers.Dense(300, use_bias=True))  # Here no softmax because we have combined it with the loss   \n",
        "        # D₁ Dropout Layer\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        # F₄ Fully-Connected Layer \n",
        "        model.add(layers.Dense(10, use_bias=True, input_shape=(10,)))  # Here no softmax because we have combined it with the loss\n",
        "        # D₂ Dropout Layer\n",
        "        model.add(layers.Dropout(0.5))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxEAD355_986"
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Enabled, as we have decided the optimal channel values\n",
        "num_ch_c1 = 90  # Question 2\n",
        "num_ch_c2 = 100  # Question 2\n",
        "\n",
        "epochs = 1000  # Fixed\n",
        "batch_size = 128  # Fixed\n",
        "learning_rate = 0.001\n",
        "\n",
        "optimizer_ = 'Adam'  # Question 3\n",
        "#use_dropout = False  # Question 3(d) (see make_model)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaGqTDBTABB1"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b2lVJwCAZLZ"
      },
      "source": [
        "if optimizer_ == 'SGD':\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "elif optimizer_ == 'SGD-momentum':  # Question 3(a)\n",
        "    momentum = 0.1\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
        "elif optimizer_ == 'RMSprop':  # Question 3(b)\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "elif optimizer_ == 'Adam':  # Question 3(c)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "else:\n",
        "    raise NotImplementedError(f'You do not need to handle [{optimizer_}] in this project.')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOHaNa2DAzmC"
      },
      "source": [
        "'''\n",
        "Load Training and Test Datasets\n",
        "'''\n",
        "x_train, y_train = load_data('/content/gdrive/My Drive/Colab Notebooks/data_batch_1')\n",
        "x_test, y_test = load_data('/content/gdrive/My Drive/Colab Notebooks/test_batch_trim')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIDI0_vRDGW0"
      },
      "source": [
        "# Create folder to store one more level down\n",
        "if not os.path.exists(f'/content/gdrive/My Drive/Colab Output/models/{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout'):\n",
        "    os.mkdir(f'/content/gdrive/My Drive/Colab Output/models/{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout')\n",
        "if not os.path.exists(f'/content/gdrive/My Drive/Colab Output/models/{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout'):\n",
        "    os.mkdir(f'/content/gdrive/My Drive/Colab Output/models/{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgn6g0gr1-b7"
      },
      "source": [
        "# Set Question Variable for file purposes\n",
        "question = 'q3c'"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5529XXDLCned"
      },
      "source": [
        "'''\n",
        "Train Model for both without and with dropout\n",
        "'''\n",
        "# Optimal Values Chosen, c1 = 90, c2 = 100\n",
        "dropout_list = [True, False]\n",
        "\n",
        "for each in dropout_list:\n",
        "\n",
        "    use_dropout = each\n",
        "    model = make_model(num_ch_c1, num_ch_c2, use_dropout) \n",
        "\n",
        "    # Train RMSprop\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics='accuracy')\n",
        "    history = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test, y_test),\n",
        "        verbose=0)\n",
        "\n",
        "    # Extract Loss Values\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    # Extract Accuracy Values\n",
        "    train_acc = history.history['accuracy']\n",
        "    test_acc = history.history['val_accuracy']\n",
        "\n",
        "    # Save instanced variables to list\n",
        "    instanced_variables = [train_loss, val_loss, train_acc, test_acc]\n",
        "\n",
        "    # Save model\n",
        "    if use_dropout:\n",
        "        model.save(f'/content/gdrive/My Drive/Colab Output/models/{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout')\n",
        "        filename = f'/content/gdrive/My Drive/Colab Output/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout_variables'\n",
        "        outfile = open(filename,'wb')\n",
        "        pickle.dump(instanced_variables, outfile)\n",
        "        outfile.close()    \n",
        "    else:\n",
        "        model.save(f'/content/gdrive/My Drive/Colab Output/models/{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout')\n",
        "        filename = f'/content/gdrive/My Drive/Colab Output/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout_variables'\n",
        "        outfile = open(filename,'wb')\n",
        "        pickle.dump(instanced_variables, outfile)\n",
        "        outfile.close()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zk3x4tuHtNV"
      },
      "source": [
        "# Retrieve previously saved DROPOUT instanced variables w/ pickle rick\n",
        "infile = open(f'/content/gdrive/My Drive/Colab Output/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout_variables', 'rb')\n",
        "retrieved_variables_dropout = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "# Retrieve previously saved NO DROPOUT instanced variables w/ pickle rick\n",
        "infile = open(f'/content/gdrive/My Drive/Colab Output/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout_variables', 'rb')\n",
        "retrieved_variables_no_dropout = pickle.load(infile)\n",
        "infile.close()\n",
        "\n",
        "# Load retrieved variables into instance variables for plotting\n",
        "train_loss_dropout, val_loss_dropout, train_acc_dropout, val_acc_dropout = retrieved_variables_dropout\n",
        "train_loss_no_dropout, val_loss_no_dropout, train_acc_no_dropout, val_acc_no_dropout = retrieved_variables_no_dropout"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGX39BSItdY1",
        "outputId": "14dcdd3e-c4cb-4680-987d-2b5774ad7581",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# get highest accuracy values\n",
        "highest_acc = max(val_acc_no_dropout)\n",
        "\n",
        "# get average accuracy\n",
        "average_acc = sum(val_acc_no_dropout)/len(val_acc_no_dropout)\n",
        "\n",
        "print('Highest Acc')\n",
        "print(highest_acc)\n",
        "\n",
        "print('')\n",
        "\n",
        "print('Avg Acc')\n",
        "print(average_acc)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest Acc\n",
            "0.10000000149011612\n",
            "\n",
            "Avg Acc\n",
            "0.10000000149011612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrNsYH9YLyqo"
      },
      "source": [
        "dropout_list = [True, False]\n",
        "for each in dropout_list:\n",
        "    \n",
        "    use_dropout = each\n",
        "    \n",
        "    if (use_dropout == True):\n",
        "        # using dropout\n",
        "        # Save the plot for loss\n",
        "        plt.plot(range(1, len(train_loss_dropout) + 1), train_loss_dropout, label='Train')\n",
        "        plt.plot(range(1, len(val_loss_dropout) + 1), val_loss_dropout, label='Test')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend()\n",
        "        plt.title(f'{optimizer_} Model Loss with dropout')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout_loss.pdf')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout_loss.png')\n",
        "        plt.close()\n",
        "        # Save the plot for accuracy (with dropout)\n",
        "        plt.plot(range(1, len(train_acc_dropout) + 1), train_acc_dropout, label='Train')\n",
        "        plt.plot(range(1, len(val_acc_dropout) + 1), val_acc_dropout, label='Test')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend()\n",
        "        plt.title(f'{optimizer_} Model Accuracy with dropout')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout_accuracy.pdf')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_dropout_accuracy.png')\n",
        "        plt.close()\n",
        "    else:\n",
        "        # no dropout\n",
        "        # Save the plot for loss\n",
        "        plt.plot(range(1, len(train_loss_no_dropout) + 1), train_loss_no_dropout, label='Train')\n",
        "        plt.plot(range(1, len(val_loss_no_dropout) + 1), val_loss_no_dropout, label='Test')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend()\n",
        "        plt.title(f'{optimizer_} Model Loss no dropout')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout_loss.pdf')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout_loss.png')\n",
        "        plt.close()\n",
        "        # Save the plot for accuracy (with dropout)\n",
        "        plt.plot(range(1, len(train_acc_no_dropout) + 1), train_acc_no_dropout, label='Train')\n",
        "        plt.plot(range(1, len(val_acc_no_dropout) + 1), val_acc_no_dropout, label='Test')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend()\n",
        "        plt.title(f'{optimizer_} Model Accuracy no dropout')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout_accuracy.pdf')\n",
        "        plt.savefig(f'/content/gdrive/My Drive/Colab Output/results/part_a_{question}_{num_ch_c1}_{num_ch_c2}_{optimizer_}_no_dropout_accuracy.png')\n",
        "        plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONMhqG_WeRR_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}